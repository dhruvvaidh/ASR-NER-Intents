{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGPCvuRwu6zN"
      },
      "source": [
        "# Automatic Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFBskQcyvbSJ"
      },
      "source": [
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xmoKHc6pXmy",
        "outputId": "08791166-ba0c-47eb-97c5-95317f7acfff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pg_0RLGowv-k"
      },
      "source": [
        "### Installing the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9Pmi2bZWuFg",
        "outputId": "d9ac1011-06ef-4784-9407-ad64586d7344"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (0.25.1)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->SpeechRecognition) (2022.12.7)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.8.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install pydub\n",
        "!pip install SpeechRecognition\n",
        "!pip install transformers\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LHndeV6woI7"
      },
      "source": [
        "### Importing the libraries and the audio file from the directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "irGusUnzl9oz"
      },
      "outputs": [],
      "source": [
        "from pydub import AudioSegment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PmtLXJMBl9kw"
      },
      "outputs": [],
      "source": [
        "audio_path = '/content/drive/MyDrive/Datasets/Dimensionless/sales_call_telephone_marketers.wav'\n",
        "sales_call_audio = AudioSegment.from_wav(audio_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71dO8yAl7gGP"
      },
      "source": [
        "## Performing ASR using Speech Recognition module (Task-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "r4ERaPZcDVrj"
      },
      "outputs": [],
      "source": [
        "from pydub.silence import split_on_silence\n",
        "\n",
        "# Set the minimum length of a segment in milliseconds\n",
        "min_segment_length = 1000\n",
        "\n",
        "# Set the minimum silence threshold in dBFS\n",
        "min_silence_threshold = -39\n",
        "\n",
        "# Split the audio into segments based on silence\n",
        "segments = split_on_silence(sales_call_audio, min_silence_len=min_segment_length, silence_thresh=min_silence_threshold)\n",
        "\n",
        "# Iterate over the segments and save each one as a separate WAV file\n",
        "for i, segment in enumerate(segments):\n",
        "    # Set the output file name and format\n",
        "    output_file = \"/content/drive/MyDrive/Datasets/Dimensionless/Alternate/\"+f'line_{i+1}.wav'\n",
        "    \n",
        "    # Save the segment as a WAV file\n",
        "    segment.export(output_file, format='wav')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9FwZI12XXArG"
      },
      "outputs": [],
      "source": [
        "import speech_recognition as sr\n",
        "recognizer = sr.Recognizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "mml3ImxiQSu_"
      },
      "outputs": [],
      "source": [
        "transcript = []\n",
        "for i in range(1,9):\n",
        "    try:\n",
        "        with sr.AudioFile(f\"/content/drive/MyDrive/Datasets/Dimensionless/Alternate/line_{i}.wav\") as source:\n",
        "                temp_audio = recognizer.record(source) \n",
        "                text = recognizer.recognize_google(audio_data =temp_audio)\n",
        "    except:\n",
        "        from pydub.effects import normalize\n",
        "        line = AudioSegment.from_file(f\"/content/drive/MyDrive/Datasets/Dimensionless/Alternate/line_{i}.wav\")\n",
        "        normalize(line).export(f\"/content/drive/MyDrive/Datasets/Dimensionless/Alternate/line{i}.wav\", format = 'wav')\n",
        "        with sr.AudioFile(f\"/content/drive/MyDrive/Datasets/Dimensionless/Alternate/line{i}.wav\") as source:\n",
        "            temp_audio = recognizer.record(source) \n",
        "            text = recognizer.recognize_google(audio_data =temp_audio)\n",
        "\n",
        "    transcript.append(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ajoi18usQSpJ",
        "outputId": "d8968837-0e4c-4ae0-8231-a77e01edab19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello hi Nancy this is Mike from AT&T incorpor',\n",
              " 'yes how can I help',\n",
              " 'Nancy you have been using our prepaid connection for a couple of years now right',\n",
              " \"yeah that's\",\n",
              " 'how would you like a postpaid connection that allows you to make free unlimited voice calls to three AT&T numbers',\n",
              " \"I would love that but what's the\",\n",
              " \"there's no catch there will be a monthly rental which you will have to pay like any other post-paid\",\n",
              " 'fantastic sign']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "transcript"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "task_1_output = '. '.join(transcript)\n",
        "task_1_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "f1f527df-203b-4075-8867-c3c6f4d51cf2",
        "id": "GnriyJCo9fo3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"hello hi Nancy this is Mike from AT&T incorpor. yes how can I help. Nancy you have been using our prepaid connection for a couple of years now right. yeah that's. how would you like a postpaid connection that allows you to make free unlimited voice calls to three AT&T numbers. I would love that but what's the. there's no catch there will be a monthly rental which you will have to pay like any other post-paid. fantastic sign\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing Entity and Intent Extraction (Task-2)"
      ],
      "metadata": {
        "id": "0LVMl22NGWam"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Dataset of custom intents "
      ],
      "metadata": {
        "id": "D50JBhVlmWLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "training_examples = [\n",
        "        ( 'My name is Jeff and I am calling from Amazon',  'intro'),\n",
        "        ( \"What's the time\",  'ask'),\n",
        "        ( 'This is a call regarding your Google Cloud Platform account.',  'purpose'),\n",
        "        (\"Amazing, count me in\",'positive'),\n",
        "        ( 'I am calling about your Microsoft Azure subscription.',  'purpose'),\n",
        "        ( 'I am calling from Tesla and my name is Elon.',  'intro'),\n",
        "        ( 'I wanted to talk to you about your Spotify plan.',  'purpose'),\n",
        "        ( \"Have you been satisfied with our subscription\",  'ask'),\n",
        "        (\"The connection allows you to use your internet  unlimited for the full month\",'inform'),\n",
        "        ( 'I am calling from Microsoft and my name is Satya.',  'intro'),\n",
        "        ( 'I would like to talk about your Amazon Web Services account.','purpose'),\n",
        "        (\"Wow, That's great\",'positive'),\n",
        "        ( \"How do you like your steak\",  'ask'),\n",
        "        ( 'I am Sundar and this is call from Google.',  'intro'),\n",
        "        (\"Your cloud account has an option of an upgrade\",'inform'),\n",
        "        (\"there will be a monthly rental which you will have to pay like any other post-paid\",'inform')\n",
        "    ]\n",
        "texts,labels = zip(*training_examples)\n",
        "intent_labels = list(set(labels))\n",
        "num_classes = len(intent_labels)\n",
        "num_classes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYqdhQ2rhU3D",
        "outputId": "b14e5c18-f408-42db-9783-75e8e22f6107"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the model and it's tokenizer"
      ],
      "metadata": {
        "id": "a1_s9AH7me52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
        "intent_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=num_classes)\n",
        "intent_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXSJwF4hox2A",
        "outputId": "81611c32-f28b-4890-c011-09e41ba0db2e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizing the input texts and making a pytorch dataset for the model"
      ],
      "metadata": {
        "id": "LB_g7-aRmrM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = intent_tokenizer(texts, padding=True, truncation=True, max_length=128, return_tensors='pt')\n",
        "classes = torch.tensor([intent_labels.index(label) for label in labels])\n",
        "dataset = torch.utils.data.TensorDataset(inputs['input_ids'], inputs['attention_mask'], classes)"
      ],
      "metadata": {
        "id": "mJ7yYFXEmq-d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model"
      ],
      "metadata": {
        "id": "2DTmZCTtoBQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(intent_model.parameters(), lr=2e-5, eps=1e-8)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True)\n",
        "intent_model.train()\n",
        "for epoch in range(3):\n",
        "    for batch in loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, classes = batch\n",
        "        outputs = intent_model(input_ids, attention_mask=attention_mask, labels=classes)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RLtoU0S9oBp-",
        "outputId": "a854765e-86c2-4b29-e1ee-d6f3c369809a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "task_3_output = []\n",
        "for text in transcript:\n",
        "    output = dict()\n",
        "    temp = dict()\n",
        "    entity = []\n",
        "    #Extracting the entities\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        temp[ent.label_] = ent.text\n",
        "    entity.append(temp)\n",
        "    #Extracting the intents\n",
        "    intent_input = intent_tokenizer.encode_plus(text,add_special_tokens=True,return_tensors='pt')\n",
        "    intent_logits = intent_model(**intent_input).logits\n",
        "    intent_id = torch.argmax(intent_logits, dim=1).item()\n",
        "    intent_label = intent_labels[intent_id]\n",
        "    #Adding it all up\n",
        "    output['sentence'] = text\n",
        "    output['intent'] = intent_label\n",
        "    output['entities'] = entity\n",
        "\n",
        "    task_3_output.append(output)"
      ],
      "metadata": {
        "id": "yLo9p50YbxT2"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_3_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AaW0mtER6X1D",
        "outputId": "b250bd80-bfea-42bd-83cc-4add57acd0b8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'sentence': 'hello hi Nancy this is Mike from AT&T incorpor',\n",
              "  'intent': 'intro',\n",
              "  'entities': [{'PERSON': 'Mike', 'ORG': 'AT&T'}]},\n",
              " {'sentence': 'yes how can I help', 'intent': 'ask', 'entities': [{}]},\n",
              " {'sentence': 'Nancy you have been using our prepaid connection for a couple of years now right',\n",
              "  'intent': 'intro',\n",
              "  'entities': [{'PERSON': 'Nancy', 'DATE': 'a couple of years'}]},\n",
              " {'sentence': \"yeah that's\", 'intent': 'intro', 'entities': [{}]},\n",
              " {'sentence': 'how would you like a postpaid connection that allows you to make free unlimited voice calls to three AT&T numbers',\n",
              "  'intent': 'ask',\n",
              "  'entities': [{'CARDINAL': 'three', 'ORG': 'AT&T'}]},\n",
              " {'sentence': \"I would love that but what's the\",\n",
              "  'intent': 'inform',\n",
              "  'entities': [{}]},\n",
              " {'sentence': \"there's no catch there will be a monthly rental which you will have to pay like any other post-paid\",\n",
              "  'intent': 'intro',\n",
              "  'entities': [{'DATE': 'monthly'}]},\n",
              " {'sentence': 'fantastic sign', 'intent': 'purpose', 'entities': [{}]}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the JSON File (Task-3)"
      ],
      "metadata": {
        "id": "12WiS7Z4y-VQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_output = dict()\n",
        "final_output['task_1_output'] = task_1_output\n",
        "final_output['task_3_output'] = task_3_output\n",
        "import json\n",
        "with open('/content/drive/MyDrive/Datasets/Dimensionless/final_output.json', 'w') as f:\n",
        "    json.dump(final_output, f)"
      ],
      "metadata": {
        "id": "r12pUpIO8l5f"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generating the summary report (Task-4)"
      ],
      "metadata": {
        "id": "b15DjxZvzIAX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sumy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2hURX4YWfiu",
        "outputId": "47269472-cb08-45b7-ab5f-3b9721d219d5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sumy in /usr/local/lib/python3.10/dist-packages (0.11.0)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.10/dist-packages (from sumy) (2.27.1)\n",
            "Requirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from sumy) (3.8.1)\n",
            "Requirement already satisfied: breadability>=0.1.20 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.1.20)\n",
            "Requirement already satisfied: pycountry>=18.2.23 in /usr/local/lib/python3.10/dist-packages (from sumy) (22.3.5)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from sumy) (0.6.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.0.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.10/dist-packages (from breadability>=0.1.20->sumy) (4.9.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.0.2->sumy) (2022.10.31)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from pycountry>=18.2.23->sumy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (1.26.15)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.7.0->sumy) (2022.12.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOswsfB5D3FR",
        "outputId": "240ff070-4f0d-406d-fff7-39367ffb72a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello hi Nancy this is Mike from AT&T incorpor.\n",
            "yes how can I help.\n",
            "Nancy you have been using our prepaid connection for a couple of years now right.\n",
            "yeah that's.\n"
          ]
        }
      ],
      "source": [
        "from sumy.parsers.plaintext import PlaintextParser\n",
        "from sumy.summarizers.luhn import LuhnSummarizer\n",
        "from sumy.nlp.tokenizers import Tokenizer\n",
        "from sumy.utils import get_stop_words\n",
        "\n",
        "# Initialize the summarizer with LuhnSummarizer\n",
        "summarizer = LuhnSummarizer()\n",
        "\n",
        "# Set summarizer parameters\n",
        "summarizer.stop_words = get_stop_words('english')\n",
        "summarizer.reduction_ratio = 0.5\n",
        "\n",
        "# Initialize parser and tokenizer\n",
        "parser = PlaintextParser.from_string(task_1_output, Tokenizer('english'))\n",
        "\n",
        "# Generate summary\n",
        "summary = summarizer(parser.document, 4) # The second argument specifies the number of sentences in the summary\n",
        "\n",
        "# Print the summary\n",
        "for sentence in summary:\n",
        "    print(sentence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "pg_0RLGowv-k",
        "7LHndeV6woI7",
        "71dO8yAl7gGP",
        "D50JBhVlmWLD",
        "a1_s9AH7me52",
        "LB_g7-aRmrM6",
        "2DTmZCTtoBQt",
        "12WiS7Z4y-VQ"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
